---
title: Preparing Documents for RAG
date: 2025-06-19
layout: post
published: false
permalink: /2017/03/windows-10-privacy/
featured_image: /assets/img/2017/2017-03-27-windows-10-privacy.jpg
excerpt: Retrieval Augmented Generation (RAG) is great and makes AI work 100% of the time ðŸ˜ (it doesn't) so do you dump your Office files into it and call it a day? Structuring the knowledge in the documents and understanding how RAG works will get you better results. It will also help you know what limitations to expect.
---

URL:
[Amass](https://github.com/OWASP/Amass#-owasp-amass)

Image:
![Views on DevOps]({{site.baseurl}}/assets/img/2018/2018-02-23-devops-ecosystem.jpg)

---

Retrievalâ€‘Augmented Generation (RAG) sounds like the perfect solution to hallucinating AIs. There is a whole [AI Incident Database](https://incidentdatabase.ai/) to make people worry. In comes RAG on its white horse, promising to back LLM output with hard data so that it doesn't make those mistakes any more.

Unfortunately, it doesn't work quite like that, despite often being sold that way to address concerns.


# RAG != LLM thinking with everything in the Knowledgebase

Let's be clear, RAG is great, and it can lead to much better outcomes. It can be implemented in a variety of ways, but the typical approach is that after you have sent your prompt to the LLM, the system will then do a semantic search of the Knowledgebase. From that, it will use a probabilistic method, usually Top-K to select chunks of the knowledgebase that are *probably* related to the query and they'll be submitted to the LLM with the prompt. The LLM then generates its output based on that, and that alone.

> User prompt â†’ Semantic / Vector Search (topâ€‘k hits) â†’ Prompt template with citations â†’ LLM â†’ Output

* **Only the *k* highestâ€‘scoring chunks** are sent to the LLM, typically 4â€‘20 passages

* **Semantic ranking** (scoring and ordering the hits) is *searchâ€‘side tuning* - it controls *what* reaches the model

* **Model fineâ€‘tuning** changes the modelâ€™s *reasoning style* or domain language. The two levers are complementary but independent

If your knowledge base is a lake, each RAG prompt draws a bucket from it, but the results aren't a distillation of the entire body of water.

# Structuring your Knowledgebase


##â€¯Chunk for context, not size
Split text so that each chunk can answer a single user question without its neighbours. You should aim for something in the 50 to 300 token range, adjusting based on what is necessary to answer the question. How big is a token? [Glad you asked](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them); about 4 characters. Depending on the RAG tools you are using, its likely that you will be able to tune the size. Azure AI Search for example suggests starting at [256 tokens](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents).



## Attach rich metadata
Your content should be annotated with metadata - file path, author, publish date, security tag - anything that lets you filter before scoring. Good metadata improves the semantic search and since its only the chunk that is sent to the LLM, it doesn't drive up your token count and costs. How you add metadata will depend on the system you are using

**3â€¯â€”â€¯Normalize and deduplicate**
Hash each chunk and skip it if you have seen the hash before. Duplicate paragraphs bloat the index and can drown out fresher content.

**4â€¯â€”â€¯Version explicitly**
Content evolves. Add a `doc_version`, `valid_from`, or similar field so old answers donâ€™t leak into new prompts. Store version info in the primary key if your vector DB supports composites.

**5â€¯â€”â€¯Separate â€œgoldâ€ from â€œbronzeâ€**
Draft or lowâ€‘confidence material shouldnâ€™t compete with your vetted corpus. Keep separate indexes, or tag drafts and filter them out at query time.

---

## 2. Reference pipeline: Azure StorageÂ â†’Â AzureÂ AIÂ Search

1. **Drop raw files**â€”PDF, DOCX, Markdown, HTML, even imagesâ€”into an **Azure Blob Storage** container.
2. An **Azure AI Search Indexer** watches that container. Builtâ€‘in skills (layout, OCR) split documents into logical *chunks* and generate embeddings via **AzureÂ OpenAI**. You can swap the default fixedâ€‘size splitter for the openâ€‘source *chunking skill* from Microsoftâ€™s PowerÂ Skills repo if you need smarter boundaries.
3. Vectors and metadata land in an **AzureÂ AI Search index**. The 2025 release added *multiâ€‘vector fields* and *semantic scoring profiles*, so you can stash both titleâ€‘level and paragraphâ€‘level embeddings and reâ€‘rank them together.
4. Your app (LangChain, Semanticâ€¯Kernel, or plain REST) sends the user question, receives the topâ€‘k hits, injects them into a prompt, and finally calls the LLM.

Swap Blob + AI Search for S3 + Kendra, or Postgres + pgvectorâ€”everything else stays the same.

---

## 3. Five dataâ€‘design principles that travel with you

**1â€¯â€”â€¯Chunk for context, not size**
Split text so that each chunk canâ€”or at least *should*â€”answer a single user question without its neighbours. Aim for something in the 50â€‘ to 300â€‘token ballpark, adjusting for your domain.

**2â€¯â€”â€¯Attach rich metadata**
File path, author, publish date, security tagâ€”anything that lets you filter before scoring. Good metadata trims false positives and speeds queries.

**3â€¯â€”â€¯Normalize and deduplicate**
Hash each chunk and skip it if you have seen the hash before. Duplicate paragraphs bloat the index and can drown out fresher content.

**4â€¯â€”â€¯Version explicitly**
Content evolves. Add a `doc_version`, `valid_from`, or similar field so old answers donâ€™t leak into new prompts. Store version info in the primary key if your vector DB supports composites.

**5â€¯â€”â€¯Separate â€œgoldâ€ from â€œbronzeâ€**
Draft or lowâ€‘confidence material shouldnâ€™t compete with your vetted corpus. Keep separate indexes, or tag drafts and filter them out at query time.

---

## 4. Updating the vector store

Stale content is worse than no contentâ€”people trust AI answers implicitly. A healthy pipeline handles four common events:

* **New or modified file** â€“ Use incremental crawling. Azure Indexers compare blob ETags or lastâ€‘modified timestamps and reâ€‘process only the changes.
* **Deleted file** â€“ Start with a softâ€‘delete flag, then schedule a hard purge. Azureâ€™s `deletionDetectionPolicy` maps missing blobs to delete actions automatically.
* **Major reâ€‘chunking (e.g., new splitter algorithm)** â€“ Reâ€‘index into a *shadow index* and flip an alias when tests pass. This is your blueâ€‘green deployment for search.
* **Embedding model upgrade** â€“ Store the model name or embedding dimension with each vector so you can phase in a new model sideâ€‘byâ€‘side. Reâ€‘embedding happens in the background without downtime.

Automation tools varyâ€”Azure Data Factory, Logic Apps, GitHub Actions, Airflow, Dagster, Prefectâ€”but the pattern remains: detect changeâ€¯â†’â€¯( reâ€‘ )â€¯processâ€¯â†’â€¯swap atomically.

---

## 5. Lowering the barrier for nonâ€‘technical contributors

* **Dragâ€‘andâ€‘drop upload** â€“ Tools like the *llamaâ€‘hub uploader* or the *RAGFlow UI* provide a simple web form that writes to your object store and posts a message to the ingest queue.
* **Content editing & review** â€“ Keep source files in Git (Markdown works great). Pull Requests double as change history and trigger your CI reâ€‘ingest pipeline.
* **Noâ€‘code pipelines** â€“ Stacks such as *LangChainâ€¯Serve* bundled with *Unstructured* let you deploy a watchâ€‘folderâ€‘toâ€‘Chroma workflow in Docker with almost zero code.
* **Lowâ€‘code search tuning** â€“ Azure AI Search exposes *Semantic Scoring Profiles* in the portal, so business owners can boost freshness or certain tags without touching vectors.
* **Governance dashboards** â€“ Haystackâ€™s knowledgeâ€‘graph extension or Semanticâ€¯Kernelâ€™s Planner show lineage, usage frequency, and orphaned chunks so editors can prune with confidence.

The guiding principle is always the same:

> *Keep raw files in object storage â†’ run an idempotent, observable ingestion job â†’ land vectorsâ€¯+â€¯metadata in a queryâ€‘optimised store â†’ surface a friendly UI so domain experts, not just engineers, own the knowledge.*

---

## Closing thoughts

A wellâ€‘structured vector store is the quiet superâ€‘power behind any successful RAG system. Whether you are on Azure, AWS, or openâ€‘source:

* **Design your chunks and metadata intentionally**â€”they decide what the model reads.
* **Treat search tuning as search tuning**; reach for model fineâ€‘tuning only when ranking canâ€™t close the gap.
* **Automate freshness** so that yesterdayâ€™s answers never embarrass tomorrowâ€™s chatbot.

Do the boring data architecture work upâ€‘front, and your LLM will sound magically wellâ€‘informed thereafter.
